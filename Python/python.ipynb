{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f3f24c",
   "metadata": {},
   "source": [
    "## Write a program that takes a string as input, and counts the frequency of each word in the string, there might be repeated characters in the string. Your task is to find the highest frequency and returns the length of the highest-frequency word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f38bbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(str):\n",
    "    w_counts = dict()\n",
    "    words = str.split()\n",
    "    for word in words:\n",
    "        if word in w_counts:\n",
    "            w_counts[word] += 1\n",
    "        else:\n",
    "            w_counts[word] = 1\n",
    "    return w_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d4274bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'“write': 1, 'write': 2, 'all': 1, 'the': 1, 'number': 1, 'from': 3, '1': 1, 'to': 1, '100”': 1}\n"
     ]
    }
   ],
   "source": [
    "print( count_words('“write write write all the number from from from 1 to 100”'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799952b6",
   "metadata": {},
   "source": [
    "# Write a program, which would download the data from the provided link, and then read the data and convert that into properly structured data and return it in Excel format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52432064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\utkar\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\utkar\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\utkar\\anaconda3\\lib\\site-packages (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\utkar\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b60176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully converted and saved to pokemon.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def download_and_convert_data(url, output_file):\n",
    "    # Download the data\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to download data from {url}\")\n",
    "        return\n",
    "\n",
    "    # Convert the data into structured format\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save the data as an Excel file\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(f\"Data successfully converted and saved to {output_file}\")\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Biuni/PokemonGO-Pokedex/master/pokedex.json\"  \n",
    "output_file = \"pokemon.xlsx\"  \n",
    "\n",
    "download_and_convert_data(url, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b65fe74",
   "metadata": {},
   "source": [
    "# Write a program to download the data from the link given below and then read the data and convert the into the proper structure and return it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b5598eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data converted and saved to nasa.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def download_and_convert_data(url, output_file):\n",
    "    # Downloading the data\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to download data from {url}\")\n",
    "        return\n",
    "\n",
    "    # Convert the data into structured format\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save the data as a CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Data converted and saved to {output_file}\")\n",
    "\n",
    "url = \"https://data.nasa.gov/resource/y77d-th95.json\"  # Replace with the actual URL\n",
    "output_file = \"nasa.csv\"  # Replace with the desired output file name\n",
    "\n",
    "download_and_convert_data(url, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18fb182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import csv\n",
    "\n",
    "# def download_and_convert_data(url, output_file):\n",
    "#     # Download the data\n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Failed to download data from {url}\")\n",
    "#         return\n",
    "\n",
    "#     # Convert the data into structured format\n",
    "#     data = response.json()\n",
    "\n",
    "#     # Save the data as a CSV file\n",
    "#     with open(output_file, 'w', newline='') as csvfile:\n",
    "#         writer = csv.DictWriter(csvfile, fieldnames=data[0].keys())\n",
    "#         writer.writeheader()\n",
    "#         writer.writerows(data)\n",
    "\n",
    "#     print(f\"Data successfully converted and saved to {output_file}\")\n",
    "\n",
    "# # Example usage\n",
    "# url = \"http://api.tvmaze.com/singlesearch/shows?q=westworld&embed=episodes\"  # Replace with the actual URL\n",
    "# output_file = \"data.csv\"  # Replace with the desired output file name\n",
    "\n",
    "# download_and_convert_data(url, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e97cf",
   "metadata": {},
   "source": [
    "# Write a program to count the number of verbs, nouns, pronouns, and adjectives in a given particular phrase or paragraph, and return their respective count as a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "924cf36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbs': 3, 'nouns': 2, 'pronouns': 2, 'adjectives': 1}\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# from nltk.tag import pos_tag\n",
    "# from collections import Counter\n",
    "\n",
    "# def count_pos_tags(text):\n",
    "#     # Tokenize the text into sentences\n",
    "#     sentences = sent_tokenize(text)\n",
    "\n",
    "#     # Initialize counters for verbs, nouns, pronouns, and adjectives\n",
    "#     verb_count = 0\n",
    "#     noun_count = 0\n",
    "#     pronoun_count = 0\n",
    "#     adjective_count = 0\n",
    "\n",
    "#     # Iterate through each sentence\n",
    "#     for sentence in sentences:\n",
    "#         # Tokenize the sentence into words\n",
    "#         words = word_tokenize(sentence)\n",
    "\n",
    "#         # Tag the words with their part-of-speech (POS) using the NLTK POS tagger\n",
    "#         tagged_words = pos_tag(words)\n",
    "\n",
    "#         # Count the number of verbs, nouns, pronouns, and adjectives in the sentence\n",
    "#         counts = Counter(tag for word, tag in tagged_words)\n",
    "\n",
    "#         verb_count += counts['VB'] + counts['VBD'] + counts['VBG'] + counts['VBN'] + counts['VBP'] + counts['VBZ']\n",
    "#         noun_count += counts['NN'] + counts['NNS'] + counts['NNP'] + counts['NNPS']\n",
    "#         pronoun_count += counts['PRP'] + counts['PRP$']\n",
    "#         adjective_count += counts['JJ'] + counts['JJR'] + counts['JJS']\n",
    "\n",
    "#     # Create a dictionary with the counts\n",
    "#     pos_counts = {\n",
    "#         'verbs': verb_count,\n",
    "#         'nouns': noun_count,\n",
    "#         'pronouns': pronoun_count,\n",
    "#         'adjectives': adjective_count\n",
    "#     }\n",
    "\n",
    "#     return pos_counts\n",
    "\n",
    "# # Example usage\n",
    "# text = \"I love eating pizza. It is delicious and satisfying.\"\n",
    "# counts = count_pos_tags(text)\n",
    "# print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21781636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbs': 2, 'nouns': 3, 'pronouns': 1, 'adjectives': 4}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def count_pos_tags(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Perform part-of-speech tagging\n",
    "    tagged_words = pos_tag(words)\n",
    "\n",
    "    # Initialize count variables\n",
    "    verb_count = 0\n",
    "    noun_count = 0\n",
    "    pronoun_count = 0\n",
    "    adjective_count = 0\n",
    "\n",
    "    # Count the number of each part of speech\n",
    "    for word, tag in tagged_words:\n",
    "        if tag.startswith('VB'):  # Verb tags start with 'VB'\n",
    "            verb_count += 1\n",
    "        elif tag.startswith('NN'):  # Noun tags start with 'NN'\n",
    "            noun_count += 1\n",
    "        elif tag.startswith('PR'):  # Pronoun tags start with 'PR'\n",
    "            pronoun_count += 1\n",
    "        elif tag.startswith('JJ'):  # Adjective tags start with 'JJ'\n",
    "            adjective_count += 1\n",
    "\n",
    "    # Create a dictionary with the counts\n",
    "    pos_counts = {\n",
    "        'verbs': verb_count,\n",
    "        'nouns': noun_count,\n",
    "        'pronouns': pronoun_count,\n",
    "        'adjectives': adjective_count\n",
    "    }\n",
    "\n",
    "    return pos_counts\n",
    "\n",
    "# Example usage\n",
    "text = \"The quick brown fox jumps over the lazy dog. He is very energetic and playful.\"\n",
    "counts = count_pos_tags(text)\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209c1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
